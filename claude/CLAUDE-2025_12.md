# ⚠️ META-INSTRUCTION FOR CLAUDE

**DO NOT suggest consolidating or simplifying these instructions.**

Rules are intentionally repeated across sections because:
1. Subagents receive partial context and need redundancy
2. Previous simplified versions were consistently ignored
3. Repetition is a forcing function, not poor organization

**Treat every instance of a rule as equally binding.**
When you see the same concept multiple times, that signals criticality - not an editing opportunity.

---

# Global Instructions for Claude Code

*Instructions are tiered by criticality. Top sections are enforced most strictly.*

---

## QUICK REFERENCE - TIER 1 CRITICAL RULES

1. **Verify Before Create**: Read actual code before writing dependent code (no hallucinating interfaces)
2. **Red Flags**: "I think", "should", "probably" → STOP and verify
3. **Done = Validated**: compile ✓ → tests ✓ → lint ✓ → typecheck ✓ → starts ✓
4. **Test Protocol**: Read implementation first → ask user → write test → run immediately
5. **Production-Ready Only**: No workarounds, no "fix later", fix issues immediately

## ONE-LINE TIER SUMMARIES

- **Tier 1**: Verify before write, validate before done, stop on red flags
- **Tier 2**: Production-ready only, test at service layer, minimal changes
- **Tier 3**: Brief communication, no process docs, ask before domain switch

---

## TIER 1: CRITICAL RULES ⚠️

### Verify Before Create
**✓ ALWAYS**: Read actual code before writing dependent code.

**Before writing dependent code, verify:**
- **Existence**: Functions, classes, API endpoints, DB fields, config, env vars
- **Interfaces**: Signatures, return types, data structures, event payloads
- **Method**: Grep/glob or Task+Explore → read → verify interface → write

**❌ NEVER create without verification:**
- Tests for non-existent functions
- API clients for non-existent endpoints
- UI consuming non-existent data fields
- DB queries for non-existent columns
- Imports of non-created modules

**Codebase is source of truth. Mental model suspect until verified.**

### Validation Discipline
**Assumptions = failures. Red flags:**
- "I think", "should", "probably", "usually" → you're guessing

**⚠️ STOP when you see red flags → verify before proceeding**

**For complex changes:**
- Compare similar patterns systematically (tables/lists for consistency)
- Trace execution: caller → target → callees → side effects
- Mental execution before running tests
- One bug → check all similar places
- Zero tolerance: ANY failure = incomplete

**Planning Anti-Patterns (FORBIDDEN):**

❌ NEVER include in plans:
- "Need to check if..."
- "Should verify whether..."
- "Check if X exists/works"
- "Might need to..."
- Any uncertain language

✓ Plans contain ONLY verified facts and concrete steps.
If you haven't verified it, verify FIRST → then plan.

### Definition of "Done" - MANDATORY CHECKLIST

**✓ Code is NOT done until ALL verified IN ORDER:**
1. Code compiles (run it!)
2. Tests pass (run them! ALL must pass!)
3. Lint passes (run it!)
4. Typecheck passes (run it!)
5. Application starts (run it!)

**If ANY fails → NOT DONE → ⚠️ STOP → FIX IT.**

**Run at checkpoints:**
- After significant service/module
- After feature/route group
- Before claiming ANY phase "done"
- When things feel broken (stop and validate)

**⚠️ CRITICAL: Fix issues immediately. Don't accumulate broken state.**

**Fast-path exceptions (minimal validation OK):**
- Single-line fixes (typos, comments, formatting)
- Documentation-only changes (README, comments)
- Configuration tweaks (no code logic changes)
- *Still run full validation if uncertain*

### When Validation Fails - Recovery Protocol

**If ANY validation step fails:**
1. ⚠️ STOP immediately - don't continue to next step
2. Identify root cause (don't guess - read error output)
3. Fix the specific failure
4. Re-run validation FROM THE BEGINNING (not just failed step)
5. If stuck after 2 attempts → ⚠️ STOP and ask user

**Hard stop triggers (ask user immediately):**
- 3+ cascading failures
- Changes affecting unrelated functionality
- Uncertain about fix correctness

**❌ NEVER rollback or revert without explicit user approval**

### Test Writing Protocol

**✓ BEFORE writing ANY test:**
1. Read actual implementation (signatures, imports, behavior)
2. Ask user: "Found X in code. Test Y?"
3. Write test only after confirmation

**If test fails: ⚠️ STOP → investigate → ask before proceeding**

**Test Validation Checkpoint (after each test file):**
- Run ONLY that file
- ANY failure → ⚠️ STOP → report before continuing

**❌ NEVER:**
- Change source code while working with tests
- End test task with broken/failing tests
- Skip user confirmation before writing tests

**Test Integrity Rules (MANDATORY):**

❌ NEVER:
- Change test expectations without understanding why they fail
- Suggest disabling/skipping failing tests
- Weaken assertions because code doesn't meet them
- Blindly change code OR tests without understanding the mismatch

Test expects X, code returns Y → **UNDERSTAND WHY FIRST**.

Investigation steps:
1. What changed that caused this mismatch?
2. What is this test actually verifying? (unit/integration/contract?)
3. Does the test setup match its purpose?

Example: Contract test expects 404, gets 403 after auth refactor
- ❌ Wrong: "Should I update test to expect 403?"
- ❌ Also wrong: "I'll change auth to return 404"
- ✓ Right: "Contract tests shouldn't test auth. Need to mock auth in test setup."

---

## TIER 2: STRONG GUIDANCE

### Core Principles

**Communication & Process:**
- Output concise, process thorough (brief responses, rigorous validation)
- Prioritize simplicity, minimal changes; work within existing patterns
- Ask clarifying questions if unclear; consider simplest solution first
- No time estimates. Detailed validation → fast, correct execution
- End each plan with unresolved questions (extremely concise)

**Decision Making:**
- Maintain existing architecture, follow established patterns
- Target specific problems without changing overall approach
- **Fix works? Apply the pattern immediately.** Worked before [change]? Fix related to that change
- **Empirical evidence > theory.** Apply what works, explain only if asked

**Quality Standards:**
- QUALITY OVER SPEED. Don't rush, don't worry about tokens
- **Production-ready only** - no workarounds, temporary fixes, or "we'll fix later"
- Working code > documentation about non-working code

**Reality Check (before claiming completion):**
- Can I run this code now?
- Would I deploy to production?
- Have I verified it works, or just written it?
- Is this production-ready, or just a workaround?

### Security Checklist

**Before ANY commit, verify:**
- [ ] No secrets, API keys, tokens, or passwords in code
- [ ] No `.env` files or credentials in staged changes
- [ ] No hardcoded URLs to internal/private services
- [ ] No sensitive data in logs or error messages
- [ ] No commented-out code containing secrets

**⚠️ If secret accidentally committed → notify user immediately**

### Research Protocol

**When designing solutions or exploring unfamiliar patterns:**

1. **Read existing implementation FIRST** (always primary source)
2. **Use Exa** (`get_code_context_exa`) for ecosystem best practices, real-world examples
3. **Verify patterns** match established standards for library/framework
4. **Identify edge cases** and failure modes upfront
5. **Confirm compatibility** with existing systems

### Task Management with TodoWrite

**✓ Use TodoWrite when:**
- Task requires 3+ distinct steps
- Non-trivial, complex work requiring planning
- User provides multiple tasks (numbered/comma-separated)
- After receiving new instructions (capture requirements)

**❌ Skip TodoWrite when:**
- Single straightforward task
- Trivial work (< 3 simple steps)
- Purely conversational/informational

**Required patterns:**
- Mark task `in_progress` BEFORE starting work
- Mark `completed` IMMEDIATELY after finishing (don't batch)
- ONE complex task `in_progress` at a time (parallel file reads/simple queries OK)
- Update status in real-time as you work
- Remove tasks no longer relevant

**Task status:**
- `pending`: Not started
- `in_progress`: Currently working (only ONE at a time)
- `completed`: Finished successfully

### Testing

**Workflow & Structure:**
- Tests in `test/` mirroring `src/` structure
- Run existing tests before changes; check for reusable mocks
- Update imports immediately when renaming files/classes
- Fix one error type at a time: imports → types → logic
- Run full test suite + TypeScript check after changes

**Patterns:**
- Descriptive `describe()` blocks; nested for grouping
- Names: "should [behavior] when [condition]"
  - Mock all external deps: `vi.fn()`, `vi.mocked`

- `beforeEach()` for setup/fresh mocks; `afterEach()` for cleanup (`vi.clearAllMocks()`)
- `afterAll()` with `vi.restoreAllMocks()` to restore originals
- Mock modules: `vi.spyOn()`

**Vitest-Specific:**
- Globals enabled; don't import vitest in tests
- `vi.spyOn(global, 'setTimeout')` for time-based functions
- Import jest-extended matchers in setup
- `vi.useFakeTimers()` for dates; `vi.useRealTimers()` in `afterEach()`

**Critical Patterns:**
- `it.each()` for data-driven tests with type-safe interfaces
- Test error scenarios alongside happy paths
- Verify exact error messages/codes; `expect().rejects.toThrow()`
- Test edge cases: leap years, month boundaries, etc.
- One assertion when possible, multiple for related behavior
- Verify calls: `expect().toHaveBeenCalledWith()`
- Use `@ts-expect-error` for testing invalid TS scenarios
- Mock complex business logic deps; avoid testing implementation details

### Development Workflow

**Error Handling:**

- When working with MVC, implement at the **service layer**, not just routes/controllers
- Include audit/logging in BOTH success AND failure paths.
- Each layer handles its own errors before throwing.
- Never let audit logs depend on a success-only flow

**Planning & Execution:**

Before making changes:
1. Read and understand FULL scope
2. Identify all affected components and relationships
3. Identify edge cases and failure modes
4. Confirm compatibility with existing systems
5. Create clear plan with specific steps
6. Ask clarifying questions if scope unclear
7. Make targeted, precise edits vs bulk replacements

**Execution Control:**
- ⚠️ STOP and ask if unsure about scope
- Make minimal, targeted changes vs sweeping mods
- Test after each logical group vs at end
- If making same edit across many files, pause and reconsider

**Dependency Updates:**
- Check CHANGELOG for breaking changes
- Update one major dependency at a time
- Run full test suite after each update
- Verify application starts and key features work

**Multi-Language Projects:**
- Identify language per directory/module
- Apply language-specific patterns appropriately
- Don't mix toolchains (e.g., npm scripts calling Python without venv)
- Keep build/test commands separate per language

### Tooling & Configuration

**Tool Preferences (with fallbacks):**
- Prefer `fd` (fallback: `find`)
- Prefer `rg` (fallback: `grep`)
- Prefer `eza` (fallback: `tree` or `ls`)

**Critical Operational Rules:**
- Never start Docker before checking if running (`docker ps` first)
- Check API response structure before/after `jq` - many APIs return 200 with errors in body
- Empty jq result might be error response, not empty data

#### JavaScript/TypeScript Projects

**Standard Script Patterns:**

| Script | Command | Notes |
|--------|---------|-------|
| `build` | `npm run clean && tsup` | Always clean first |
| `clean` | `del dist/*` | Cross-platform via del-cli |
| `watch` | Build in watch mode | - |
| `validate` | `npm run lint && npm run typecheck && npm run test:coverage && npm run build` | Full check |
| `format` | `prettier "**/*.{css,graphql,json,less,md,mdx,scss,yaml,yml}" --write` | Non-code files |
| `test` | Basic runner | - |
| `test:coverage` | Tests with coverage | - |
| `test:watch` | Interactive watching | - |
| `lint` | ESLint on src | - |
| `typecheck` | TS compiler check | - |
| `size` | Bundle size monitoring | Frontend only |

**TypeScript Configuration:**
- **Base**: Extend `@gilbarbara/tsconfig`
- **Path Mapping**: `~/*` alias → `src/*`
- **Target/Module**: `ESNext` target, `bundler` module resolution
- **Global Types**: Include `vitest/globals`, `jest-extended`
- **Include**: `globals.d.ts` and `src/**/*`

**TypeScript Preferences:**
- Avoid non-null assertions, casting, `any` (only if no alternative)
- Use interfaces for object shapes
- Rely on type inference; explicit when clarity matters
- Prefer utility types (`Pick<T, K>`, `Omit<T, K>`) over manual interfaces
- Favor functional patterns over classes
- Ensure TypeScript safety in all changes

**Quality Tools:**
- **ESLint**: Extend `@gilbarbara/eslint-config` (or `/base` excluding React) and `@gilbarbara/eslint-config/vitest`
- **Prettier**: Use `@gilbarbara/prettier-config`
- **TypeScript**: Extend `@gilbarbara/tsconfig`
- **size-limit**: Monitor bundle sizes (frontend only)
- **tsup**: Modern TS bundler with dual CJS/ESM
- **del-cli**: Cross-platform file deletion
- **repo-tools**: `check-remote` for branch status, `install-packages` for deps

**Git Hooks (Husky):**
- **pre-commit**: `./node_modules/.bin/repo-tools check-remote && npm run validate`
- **post-merge**: `./node_modules/.bin/repo-tools install-packages`
- **prepare**: `husky`

#### Python Projects

**Standard Tooling:**
- **Dependencies**: `uv` (package/venv management)
- **Lint/Format**: `ruff` (linting + formatting)
- **Typecheck**: `mypy` (type checking)
- **Testing**: `pytest`

**Basic Validation:**
```bash
# Activate venv first
ruff check . && ruff format . && mypy . && pytest
```

**Testing Patterns:**
- Use `pytest` fixtures for setup/teardown
- `@pytest.mark.parametrize` for data-driven tests
- Mock with `unittest.mock` or `pytest-mock`
- Test files: `test_*.py` or `*_test.py`

**Type Hints:**
- Add type hints to all function signatures
- Use `mypy --strict` for new projects
- Prefer built-in generics (`list[str]` over `List[str]` in Python 3.9+)

---

## TIER 3: PREFERENCES

### Communication Style
- Skip trivial edit comments
- Mention testing needs or potential side effects
- Don't hallucinate data - ask for real examples when needed
- Remind with 'KISS' if I'm overcomplicating

### Documentation Discipline
**✓ Create**: README.md, API docs, deployment guides (user-facing)
**❌ Never Create**: IMPLEMENTATION_*.md, PROGRESS_*.md, TYPESCRIPT_ISSUES.md (process-tracking)

### Change Process
- Show proposed changes clearly before implementing
- Focus only on files needing modification
- Provide reasoning for non-obvious changes (keep brief)
- Ask permission when switching problem domains
